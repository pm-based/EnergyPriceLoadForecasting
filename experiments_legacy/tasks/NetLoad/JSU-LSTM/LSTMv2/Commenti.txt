Con batch = 16…32, lag_win = 1…3, e lr = 0.005… 0.0005: il training risulta molto instabile, spesso gli spike impedisco alla loss di scendere abbastanza in fretta e si attiva l’es. Questo genera winskler e pinball molto altri.
Con batch 256 (e reg l1 e l2 1e-5, n_val 300) il training è molto più costante, ma i risultati sono molto simili.
Con batch 1024 (n_val 300, es = 80, epoche = 1000) curva del training molto più smooth ma non sempre decrescente. I risultati sono sempre gli stessi pessimi. In più guardare l’immagine nella cartella
Con reg 1e-4 il risultato non cambia…

IL PROBLEMA è CHE PER L’ORA 9 LA DISTRIBUZIONE HA PARAMETRI TALI CHE LA JOHNSON SU VALE INF!
Il problema non è dunque il training in se.
Il modello in XX - batch128+LSTMdropout0.2_0.2+l2_1e-2+lr0 ha un delta score di 0.32


Soluzione: aumentare il day_lag risolve in gran parte il problema. Con 7 è già buono.

Con batch 128, 2 layers (256 e 128), reg l2 1e-4, max epoch 800, e dropoutLSTM0.2 con 1 di day_lag
Funziona bene ma nel grafico ci sono quantili con spike verso il basso con alcune che vanno sotto zero. I risultati sono generalmente buoni.
Fatto solo su un ensable! Ci mette un pò

FIN ORA SI è SCOPERTO:
Da riprovare i due layers, e vedere se effettivamente mitiga il problema. La combinazione più layers con più giorni potrebbe aiutare molto


Test con softmax lr 0.0005 e l2 1e-2 peggiora i risultati. Risultato molto irregolare

Test con 2 layers (512, 128) con dropoutLSTM 0.2 solo sul primo batch 256 Davvero pessimo. Stessi problemi NAN o INF. Il motivo è che c’è stata una spike durante il training. Con lag = 1 a quanto pare il problema è proprio strutturale.

Con preprocessing (cyc e 1log) e 7 giorni di lag buono!
Con preprocessing (cyc e 1log) e 1 giorni di lag davvero pessimo, il peggiore pinball e wink = inf
Con preprocessing (cyc e ArcSinh) e 1 giorno di lag davvero pessimo, il peggiore pinball e wink = inf


SOLUZIONE!
modificare gli argomenti di tfd.JohnsonSU in modo che tailweight non sia negativa (usando ReLu):
 tailweight=1e-3 + tf.nn.relu(t[..., self.settings['pred_horiz']:2 * self.settings['pred_horiz']]),

Modifiche gli argomenti di JSU e le loro conseguenze:
    	tailweight e scale solo positivi
    	tailweight=1e-3 + tf.nn.relu(
    	scale=1e-3 + tf.nn.relu(
       		Ottengo tailwight tra 2 e 3 e scale tra 0.1 e 0.2
        		Delta cov (3.8-2.7), pinball 4.7 winskler (224-277)
	
	tailweight e scale solo positivi e inizializzati con la media
    	tailweight=1e-3 + tf.nn.relu( 2 +
    	scale=1e-3 + tf.nn.relu( 0.17 +
		Ottengo tailwight circa 4 e scale circa 0.26 
        		Delta cov (5-1.7), pinball 5.7 winskler (226)
	SEMBRA ESSERCI UN Pò DI ALEATORIETà CON IL DELTA COV….

	tailweight positivo e scale a caso
    	tailweight=1e-3 + tf.nn.relu(
    	scale= …
	DA FARE
		

	
	
